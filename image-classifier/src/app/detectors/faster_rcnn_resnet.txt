import cv2
import dataclasses
from typing import Dict, List, Tuple, Any
import numpy as np
import copy
import os
import requests
import boto3
import logging
import torchvision
from typing import Callable
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.rpn import AnchorGenerator

from .common import BaseObjectDetector

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)



def create_model():
    # Load the pretrained SqueezeNet1_0 backbone.
    weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1
    backbone = torchvision.models.resnet50(
        weights=weights
    ) #.features

    num_classes = len(weights.meta["categories"])
    # print(num_classes)

    # We need the output channels of the last convolutional layers from
    # the features for the Faster RCNN model.
    # It is 512 for SqueezeNet1_0.
    backbone.out_channels = 512
    # Generate anchors using the RPN. Here, we are using 5x3 anchors.
    # Meaning, anchors with 5 different sizes and 3 different aspect 
    # ratios.
    anchor_generator = AnchorGenerator(
        sizes=((32, 64, 128, 256, 512),),
        aspect_ratios=((0.5, 1.0, 2.0),)
    )
    # Feature maps to perform RoI cropping.
    # If backbone returns a Tensor, `featmap_names` is expected to
    # be [0]. We can choose which feature maps to use.
    roi_pooler = torchvision.ops.MultiScaleRoIAlign(
        featmap_names=['0'],
        output_size=7,
        sampling_ratio=2
    )
    # Final Faster RCNN model.
    model = FasterRCNN(
        backbone=backbone,
        weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1,
        # num_classes=num_classes,
        rpn_anchor_generator=anchor_generator,
        box_roi_pool=roi_pooler,
        box_predictor=FastRCNNPredictor(1024, num_classes)
    )
    # print(model)
    return model


def draw_boxes(boxes, classes, image, color):
    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)

    for i, box in enumerate(boxes):
        cv2.rectangle(
            image,
            (int(box[0]), int(box[1])),
            (int(box[2]), int(box[3])),
            color, 2
        )

        cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,
                    lineType=cv2.LINE_AA)

    return image
 

# @dataclass
# class TorchVisionModel:
#     model_init_function: Callable[[], torchvision.models.detection.faster_rcnn.FasterRCNN]
#     model_weights: torchvision.models.detection.WeightsEnum


@dataclasses.dataclass
class FasterRCNNResNetDetector(BaseObjectDetector):
    bounding_box_color: Tuple[int, int, int] = (255, 0, 0)
    detector_name: str = "faster_rcnn_resnet"
    detector_version: str = "0.0.1"

    # model_init_function = create_model #torchvision.models.detection.fasterrcnn_resnet50_fpn_v2
    # model_weights = torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1

    def __post_init__(self):
        self.confidence_limit = 0.85

        # self.dataset_labels = torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1.meta["categories"]
        self.dataset_labels = torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1.meta["categories"]


        # https://debuggercafe.com/using-any-torchvision-pretrained-model-as-backbone-for-pytorch-faster-rcnn/
        # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)
        # model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)

        self.model = create_model()

        self.model.eval().to('cpu')

        self.transform_image = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
        ])

    def process(self, image: np.array, **kwargs) -> List["ProcessedImage"]:
        # transform the image to tensor
        transformed_image = self.transform_image(image)
        transformed_image = transformed_image.unsqueeze(0) # add a batch dimension

        outputs = self.model(transformed_image) # get the predictions on the image
        # print(outputs)
        assert len(outputs) == 1
        boxes, labels, scores = [outputs[0][x] for x in ['boxes', 'labels', 'scores']]

        mask = scores >= self.confidence_limit

        boxes, labels, scores = [x.detach().cpu().numpy()[mask] for x in [boxes, labels, scores]]

        print(boxes)
        print(labels)
        label_names = [self.dataset_labels[i] for i in labels]

        image = draw_boxes(boxes, label_names, image, self.bounding_box_color)

        object_names = set(label_names)
        label_names_lookup = zip(label_names, boxes)

        from ..records import ProcessedImage
        processed_images = []
        for object_name in object_names:
            object_boxes = [box for label_name, box in label_names_lookup if label_name == object_name]
            processed_images.append(
                ProcessedImage(
                    image=image,
                    object_name=object_name,
                    object_bounding_boxes=[(int(x), int(y), int(w), int(h)) for (x, y, w, h) in object_boxes], 
                    detector_name=self.detector_name,
                    detector_version=self.detector_version,
                    **kwargs
                )
            )
            
        return processed_images
    

 

