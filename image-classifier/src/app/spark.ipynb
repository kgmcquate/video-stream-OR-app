{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf4ffc2-7885-4f88-8e3c-696128c7937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any\n",
    "\n",
    "# from pytube import YouTube\n",
    "from urllib.parse import urlunsplit, urlencode\n",
    "from pydantic.dataclasses import dataclass\n",
    "import dataclasses\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import socket\n",
    "\n",
    "# import libraries\n",
    "# from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fastavro\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, BinaryType\n",
    "\n",
    "from fastavro import writer, reader, parse_schema\n",
    "import io\n",
    "\n",
    "from classifiers import RawImageRecord\n",
    "\n",
    "from kafka import kafka_config, raw_video_frames_topic_name, processed_video_frames_topic_name\n",
    "\n",
    "from classifiers import HAARClassifier, BaseObjectDetector, ProcessedImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675659ca-49e7-4393-8db1-4f14eb5e0a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 7d0e8807-e171-4f45-8def-7701cd57c9a2, runId = bc46a35c-d5e3-47df-925b-d0098e91f5f2] terminated with exception: Failed to create new KafkaAdminClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 87\u001b[0m\n\u001b[1;32m     59\u001b[0m stream \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     spark\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m.\u001b[39mreadStream\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# time.sleep(100) # sleep 10 seconds\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# stream.stop()\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# dir(stream)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# stream.show()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# main()\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 7d0e8807-e171-4f45-8def-7701cd57c9a2, runId = bc46a35c-d5e3-47df-925b-d0098e91f5f2] terminated with exception: Failed to create new KafkaAdminClient"
     ]
    }
   ],
   "source": [
    "\n",
    "avro_schema = \"\"\"\n",
    "        {\n",
    "            \"type\": \"record\",\n",
    "            \"namespace\": \"com.mycorp.mynamespace\",\n",
    "            \"name\": \"sampleRecord\",\n",
    "            \"doc\": \"Sample schema to help you get started.\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"video_stream_id\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"doc\": \"id for video stream taken from source\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"jpeg_image\",\n",
    "                    \"type\": \"bytes\",\n",
    "                    \"doc\": \"jpeg image\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"metadata_json\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"doc\": \"Any additional information in json format\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bytes_with_schema_to_avro(binary, avro_read_schema=parse_schema(json.loads(avro_schema))):\n",
    "    with io.BytesIO(binary) as bytes_io:\n",
    "        reader = fastavro.reader(bytes_io, avro_read_schema)\n",
    "        return next(reader)\n",
    "\n",
    "# def main():\n",
    "\n",
    "    \n",
    "# .config(\"spark.jars\", \"spark-sql-kafka-0-10_2.12-3.5.0.jar\")\\\n",
    "# .config(\"spark.driver.extraClassPath\", \"spark-sql-kafka-0-10_2.12-3.5.0.jar\")\\\n",
    "# .config(\"spark.executor.extraClassPath\", \"spark-sql-kafka-0-10_2.12-3.5.0.jar\")\\\n",
    " \n",
    "# .config(\"spark.jars\", )\\\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".config(\n",
    "    \"spark.jars\",\n",
    "    ','.join([\n",
    "        \"https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar\",\n",
    "        \"https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar\",\n",
    "        \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.5.0/spark-streaming-kafka-0-10-assembly_2.12-3.5.0.jar\",\n",
    "        \"https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar\",\n",
    "        \"https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar\"\n",
    "    ])\n",
    ")\\\n",
    ".getOrCreate()\n",
    "\n",
    "stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(\n",
    "        **{f\"kafka.{k}\": v for k, v in kafka_config.items()}\n",
    "    )\n",
    "    .option(\"subscribe\", raw_video_frames_topic_name)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .select(\"key\", \"value\")\n",
    "    .writeStream.format(\"console\").start()\n",
    "    # .rdd\n",
    "    # .start()\n",
    "    # .mapValues(bytes_with_schema_to_avro)\n",
    "    # .map(lambda key, record: RawImageRecord(key=key, **record))\n",
    "    # .flatMapValues(lambda record: record.process_image())\n",
    "    # .map(lambda key, image: {\n",
    "    #         \"key\": f\"{key}_{image.detector_name}_{image.object_name}\",\n",
    "    #         \"value\": image.to_json()\n",
    "    #     }\n",
    "    # )\n",
    ")\n",
    "\n",
    "# import time\n",
    "# time.sleep(100) # sleep 10 seconds\n",
    "# stream.stop()\n",
    "\n",
    "stream.awaitTermination()\n",
    "\n",
    "# dir(stream)\n",
    "\n",
    "# stream.show()\n",
    "\n",
    "# write_schema = StructType([\n",
    "#     StructField(\"key\", StringType()),\n",
    "#     StructField(\"value\", StringType())\n",
    "# ])\n",
    "\n",
    "# (\n",
    "#     spark.createDataFrame(stream, schema=write_schema)\n",
    "#     .writeStream\n",
    "#     .format('kafka')\n",
    "#     .options(\n",
    "#         **{f\"kafka.{k}\": v for k, v in kafka_config.items()}\n",
    "#     )\n",
    "    \n",
    "#     .option(\"topic\", processed_video_frames_topic_name)\n",
    "#     # .option(\"checkpointLocation\", \"/path/to/checkpoint/dir/in/hdfs/\")\n",
    "#     # .trigger()\n",
    "#     .save()\n",
    "# )\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2534d157-1ff5-428c-9c93-032a967bf1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap.servers': 'pkc-p11xm.us-east-1.aws.confluent.cloud:9092',\n",
       " 'security.protocol': 'SASL_SSL',\n",
       " 'sasl.mechanism': 'PLAIN',\n",
       " 'sasl.username': 'ZMZ76UXSHPTUA5HO',\n",
       " 'sasl.password': 'd4j2dCdXsgHyFAY15haq0mBJMoQ+bqPeXlvafabiJtjFjij5VEuy+Dp6r2VD3p3I',\n",
       " 'client.id': '188ee4ecd2b6'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920797a9-478e-4404-a3bd-688834934000",
   "metadata": {},
   "outputs": [],
   "source": [
    "    {\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\" : \"SASL_SSL\",\n",
    "    \"kafka.bootstrap.servers\": bootstrap_servers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb958a7-77a9-41fa-a3a3-d327b212b10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kafka.bootstrap.servers': 'pkc-p11xm.us-east-1.aws.confluent.cloud:9092',\n",
       " 'kafka.security.protocol': 'SASL_SSL',\n",
       " 'kafka.sasl.mechanism': 'PLAIN',\n",
       " 'kafka.sasl.username': 'ZMZ76UXSHPTUA5HO',\n",
       " 'kafka.sasl.password': 'd4j2dCdXsgHyFAY15haq0mBJMoQ+bqPeXlvafabiJtjFjij5VEuy+Dp6r2VD3p3I',\n",
       " 'kafka.client.id': '188ee4ecd2b6'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{f\"kafka.{k}\": v for k, v in kafka_config.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
