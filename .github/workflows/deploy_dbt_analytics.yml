name: Deploy DBT Analytics

on:
  push:
    branches: [ "main" ]
    paths:
      - dbt_analytics/**

env:
  AWS_REGION: "us-east-1"
  AWS_ACCOUNT_ID: "117819748843"
  AWS_ACCESS_KEY_ID: "${{ secrets.AWS_ACCESS_KEY_ID }}"
  AWS_SECRET_ACCESS_KEY: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"

defaults:
 run:
  working-directory: dbt_analytics/

permissions:
  contents: read

jobs:
  deploy-dbt_analytics:
    name: Deploy dbt_analytics Airflow DAG
    runs-on: ubuntu-latest
    environment: production
    # container:
    #   image: public.ecr.aws/sam/build-python3.7:latest #continuumio/miniconda3 #public.ecr.aws/sam/build-python3.9:latest #continuumio/miniconda3 #public.ecr.aws/sam/build-python3.7:latest #public.ecr.aws/sam/build-python3.9:latest #amazonlinux:2 #

    steps:
      - name: Checkout
        uses: taiki-e/checkout-action@v1

      - name: Copy to Airflow S3
        run: |
          aws s3 cp --recursive ./  s3://deployment-zone-${AWS_ACCOUNT_ID}-${AWS_REGION}/airflow/dags/
